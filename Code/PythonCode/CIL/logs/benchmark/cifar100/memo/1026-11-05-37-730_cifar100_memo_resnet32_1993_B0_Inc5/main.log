2023-10-26 11:05:37,731 [trainer.py] => Time Str >>> 1026-11-05-37-730
2023-10-26 11:05:37,731 [trainer.py] => memory_per_class: 20
2023-10-26 11:05:37,731 [trainer.py] => fixed_memory: False
2023-10-26 11:05:37,731 [trainer.py] => shuffle: True
2023-10-26 11:05:37,731 [trainer.py] => model_name: memo
2023-10-26 11:05:37,731 [trainer.py] => seed: 1993
2023-10-26 11:05:37,731 [trainer.py] => dataset: cifar100
2023-10-26 11:05:37,731 [trainer.py] => init_cls: 5
2023-10-26 11:05:37,731 [trainer.py] => increment: 5
2023-10-26 11:05:37,731 [trainer.py] => convnet_type: memo_resnet32
2023-10-26 11:05:37,731 [trainer.py] => prefix: benchmark
2023-10-26 11:05:37,731 [trainer.py] => device: [device(type='cpu')]
2023-10-26 11:05:37,732 [trainer.py] => debug: False
2023-10-26 11:05:37,732 [trainer.py] => skip: False
2023-10-26 11:05:37,732 [trainer.py] => train_base: True
2023-10-26 11:05:37,732 [trainer.py] => train_adaptive: False
2023-10-26 11:05:37,732 [trainer.py] => scheduler: steplr
2023-10-26 11:05:37,732 [trainer.py] => init_epoch: 200
2023-10-26 11:05:37,732 [trainer.py] => init_milestones: ['6', '0', ',', '1', '2', '0', ',', '1', '7', '0']
2023-10-26 11:05:37,732 [trainer.py] => t_max: None
2023-10-26 11:05:37,732 [trainer.py] => epochs: 170
2023-10-26 11:05:37,732 [trainer.py] => milestones: [80, 120, 150]
2023-10-26 11:05:37,732 [trainer.py] => lrate_decay: 0.1
2023-10-26 11:05:37,732 [trainer.py] => batch_size: 128
2023-10-26 11:05:37,732 [trainer.py] => lrate: 0.1
2023-10-26 11:05:37,732 [trainer.py] => weight_decay: 0.0002
2023-10-26 11:05:37,732 [trainer.py] => alpha_aux: 1.0
2023-10-26 11:05:37,732 [trainer.py] => config: ./exps/memo.json
2023-10-26 11:05:37,732 [trainer.py] => time_str: 1026-11-05-37-730
2023-10-26 11:05:37,732 [trainer.py] => exp_name: 1026-11-05-37-730_cifar100_memo_resnet32_1993_B0_Inc5
2023-10-26 11:05:37,732 [trainer.py] => logfilename: logs/benchmark/cifar100/memo/1026-11-05-37-730_cifar100_memo_resnet32_1993_B0_Inc5
2023-10-26 11:05:37,732 [trainer.py] => csv_name: cifar100_1993_memo_resnet32_B0_Inc5
2023-10-26 11:05:39,396 [inc_net.py] => Task Agnostic Extractor is: <class 'convs.memo_cifar_resnet.GeneralizedResNet_cifar'>
2023-10-26 11:05:39,397 [inc_net.py] => Task agnostic extractor structure: GeneralizedResNet_cifar(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResnetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
2023-10-26 11:05:39,397 [memo.py] => >>> train generalized blocks:True train_adaptive: False
2023-10-26 11:05:39,397 [trainer.py] => Type of model is: <class 'models.memo.MEMO'>
2023-10-26 11:05:39,397 [trainer.py] => Type of model network is: <class 'utils.inc_net.AdaptiveNet'>
2023-10-26 11:05:39,397 [trainer.py] => Start time:1698293139.39746
2023-10-26 11:05:39,397 [trainer.py] => Data manager.nb_tasks is: 20
2023-10-26 11:05:39,397 [trainer.py] => All params: 112016
2023-10-26 11:05:39,397 [trainer.py] => Trainable params: 112016
2023-10-26 11:05:39,397 [inc_net.py] => Calling function update_fc from Adaptive net class
2023-10-26 11:05:39,406 [inc_net.py] => Get extractor
2023-10-26 11:05:39,406 [inc_net.py] => SpecializedResNet_cifar(
  (final_stage): Sequential(
    (0): ResnetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResnetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResnetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResnetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResnetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
)
2023-10-26 11:05:39,406 [inc_net.py] => out dim is: None
2023-10-26 11:05:39,406 [inc_net.py] => out dim is: 64
2023-10-26 11:05:39,406 [memo.py] => Learning on 0-5
2023-10-26 11:05:39,406 [memo.py] => All params: 464219
2023-10-26 11:05:39,407 [memo.py] => Trainable params: 464219
2023-10-26 11:06:45,925 [memo.py] => Task 0, Epoch 1/200 => Loss 0.191, Train_accy 20.31, Test_accy 20.00
2023-10-26 11:07:43,368 [memo.py] => Task 0, Epoch 1/200 => Loss 2.857, Train_accy 17.97, Test_accy 20.00
2023-10-26 11:08:40,720 [memo.py] => Task 0, Epoch 1/200 => Loss 16620143786182017932066816.000, Train_accy 19.53, Test_accy 20.00
2023-10-26 11:09:38,240 [memo.py] => Task 0, Epoch 1/200 => Loss nan, Train_accy 19.14, Test_accy 20.00
2023-10-26 11:10:35,325 [memo.py] => Task 0, Epoch 1/200 => Loss nan, Train_accy 18.59, Test_accy 20.00
2023-10-26 11:11:32,423 [memo.py] => Task 0, Epoch 1/200 => Loss nan, Train_accy 18.88, Test_accy 20.00
2023-10-26 11:12:29,506 [memo.py] => Task 0, Epoch 1/200 => Loss nan, Train_accy 19.87, Test_accy 20.00
2023-10-26 11:13:25,898 [memo.py] => Task 0, Epoch 1/200 => Loss nan, Train_accy 19.73, Test_accy 20.00
