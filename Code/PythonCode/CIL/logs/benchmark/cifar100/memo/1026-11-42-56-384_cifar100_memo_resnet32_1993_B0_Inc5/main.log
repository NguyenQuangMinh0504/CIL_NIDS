2023-10-26 11:42:56,384 [trainer.py] => Time Str >>> 1026-11-42-56-384
2023-10-26 11:42:56,385 [trainer.py] => memory_per_class: 20
2023-10-26 11:42:56,385 [trainer.py] => fixed_memory: False
2023-10-26 11:42:56,385 [trainer.py] => shuffle: True
2023-10-26 11:42:56,385 [trainer.py] => model_name: memo
2023-10-26 11:42:56,385 [trainer.py] => seed: 1993
2023-10-26 11:42:56,385 [trainer.py] => dataset: cifar100
2023-10-26 11:42:56,385 [trainer.py] => init_cls: 5
2023-10-26 11:42:56,385 [trainer.py] => increment: 5
2023-10-26 11:42:56,385 [trainer.py] => convnet_type: memo_resnet32
2023-10-26 11:42:56,385 [trainer.py] => prefix: benchmark
2023-10-26 11:42:56,385 [trainer.py] => device: [device(type='cpu')]
2023-10-26 11:42:56,385 [trainer.py] => debug: False
2023-10-26 11:42:56,385 [trainer.py] => skip: False
2023-10-26 11:42:56,385 [trainer.py] => train_base: True
2023-10-26 11:42:56,385 [trainer.py] => train_adaptive: False
2023-10-26 11:42:56,385 [trainer.py] => scheduler: steplr
2023-10-26 11:42:56,385 [trainer.py] => init_epoch: 200
2023-10-26 11:42:56,385 [trainer.py] => init_milestones: [60, 120, 170]
2023-10-26 11:42:56,385 [trainer.py] => t_max: None
2023-10-26 11:42:56,385 [trainer.py] => epochs: 170
2023-10-26 11:42:56,385 [trainer.py] => milestones: [80, 120, 150]
2023-10-26 11:42:56,385 [trainer.py] => lrate_decay: 0.1
2023-10-26 11:42:56,385 [trainer.py] => batch_size: 128
2023-10-26 11:42:56,385 [trainer.py] => lrate: 0.1
2023-10-26 11:42:56,385 [trainer.py] => weight_decay: 0.0002
2023-10-26 11:42:56,385 [trainer.py] => alpha_aux: 1.0
2023-10-26 11:42:56,385 [trainer.py] => config: ./exps/memo.json
2023-10-26 11:42:56,385 [trainer.py] => time_str: 1026-11-42-56-384
2023-10-26 11:42:56,385 [trainer.py] => exp_name: 1026-11-42-56-384_cifar100_memo_resnet32_1993_B0_Inc5
2023-10-26 11:42:56,386 [trainer.py] => logfilename: logs/benchmark/cifar100/memo/1026-11-42-56-384_cifar100_memo_resnet32_1993_B0_Inc5
2023-10-26 11:42:56,386 [trainer.py] => csv_name: cifar100_1993_memo_resnet32_B0_Inc5
2023-10-26 11:42:58,010 [inc_net.py] => Task Agnostic Extractor is: <class 'convs.memo_cifar_resnet.GeneralizedResNet_cifar'>
2023-10-26 11:42:58,010 [inc_net.py] => Task agnostic extractor structure: GeneralizedResNet_cifar(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResnetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResnetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResnetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
2023-10-26 11:42:58,011 [memo.py] => >>> train generalized blocks:True train_adaptive: False
2023-10-26 11:42:58,011 [trainer.py] => Type of model is: <class 'models.memo.MEMO'>
2023-10-26 11:42:58,011 [trainer.py] => Type of model network is: <class 'utils.inc_net.AdaptiveNet'>
2023-10-26 11:42:58,011 [trainer.py] => Start time:1698295378.0112221
2023-10-26 11:42:58,011 [trainer.py] => Data manager.nb_tasks is: 20
2023-10-26 11:42:58,011 [trainer.py] => All params: 112016
2023-10-26 11:42:58,011 [trainer.py] => Trainable params: 112016
2023-10-26 11:42:58,011 [inc_net.py] => ----------------------------------------------------
2023-10-26 11:42:58,011 [inc_net.py] => Calling function update_fc from Adaptive net class...
2023-10-26 11:42:58,011 [inc_net.py] => Updating fully connected layer...
2023-10-26 11:42:58,020 [inc_net.py] => Current fc is: None
2023-10-26 11:42:58,020 [inc_net.py] => Generating fully connected layer with in_dim 64, out_dim 5
2023-10-26 11:42:58,020 [inc_net.py] => New fc is: SimpleLinear()
2023-10-26 11:42:58,020 [memo.py] => Learning on 0-5
2023-10-26 11:42:58,020 [memo.py] => All params: 464219
2023-10-26 11:42:58,020 [memo.py] => Trainable params: 464219
2023-10-26 11:43:08,238 [memo.py] => tensor([1, 1, 0, 2, 0, 4, 2, 2, 1, 4, 1, 2, 4, 2, 0, 1, 1, 3, 3, 0, 0, 4, 3, 1,
        4, 3, 4, 2, 3, 1, 2, 3, 1, 2, 4, 0, 4, 4, 1, 4, 2, 0, 2, 2, 3, 2, 1, 0,
        1, 4, 0, 1, 3, 0, 3, 1, 1, 1, 0, 0, 2, 1, 4, 4, 3, 0, 2, 4, 0, 3, 2, 1,
        1, 4, 2, 2, 2, 3, 4, 1, 4, 3, 0, 4, 3, 1, 0, 2, 4, 4, 0, 1, 0, 3, 3, 2,
        0, 2, 0, 0, 1, 3, 1, 3, 2, 4, 3, 3, 4, 4, 4, 3, 2, 3, 2, 1, 4, 4, 2, 3,
        4, 3, 3, 3, 0, 2, 2, 1])
2023-10-26 11:43:08,240 [memo.py] => torch.Size([128])
2023-10-26 11:43:08,597 [memo.py] => tensor([[-1.0790e+00,  4.4053e+00, -9.9330e-01, -1.8891e+00,  2.1221e-01],
        [-6.6863e-01,  3.1648e+00, -6.6132e-01, -9.2515e-01, -2.3272e-01],
        [-1.2268e+00,  7.5766e+00, -6.4280e-01, -3.3316e+00,  1.1739e+00],
        [-7.0658e-01,  4.0135e+00, -7.8544e-01, -1.4066e+00,  6.5716e-01],
        [-6.9867e-01,  3.9947e+00, -7.4381e-01, -1.3135e+00,  5.8047e-01],
        [-6.3827e-01,  2.3334e+00, -6.6523e-01, -7.2526e-01, -2.3827e-01],
        [-7.2042e-01,  5.8094e+00, -6.4807e-01, -2.1288e+00,  4.2834e-01],
        [-1.0851e+00,  5.7401e+00, -7.1323e-01, -2.3310e+00, -3.6522e-01],
        [-8.6624e-01,  4.6196e+00, -5.9436e-01, -1.9634e+00,  2.3811e-01],
        [-2.2027e-01,  3.7972e+00, -9.2138e-01, -1.3146e+00, -4.5852e-01],
        [-1.0344e+00,  2.8186e+00, -8.3807e-01, -8.3663e-01,  4.9135e-02],
        [-3.2421e-01,  4.0030e+00, -6.4796e-01, -1.2311e+00,  2.1131e-03],
        [-9.4498e-01,  2.5495e+00, -4.8410e-01, -1.1555e-01, -2.3126e-01],
        [-9.6707e-01,  5.5605e+00, -4.9870e-01, -1.9113e+00,  7.2592e-01],
        [-6.1126e-01,  4.3869e+00, -6.3469e-01, -1.4540e+00,  3.8298e-01],
        [-7.9699e-01,  4.9394e+00, -4.9660e-01, -1.6552e+00,  4.5721e-01],
        [-7.9925e-01,  4.2191e+00, -3.8939e-01, -1.6812e+00, -1.6838e-01],
        [-6.6428e-01,  2.8937e+00, -6.1688e-01, -8.9277e-01, -1.6181e-01],
        [-4.8869e-01,  3.6917e+00, -7.8784e-01, -1.3127e+00,  2.3845e-01],
        [-7.8048e-01,  6.2003e+00, -9.9673e-01, -2.4387e+00, -1.3432e-01],
        [-1.7558e-01,  4.5092e+00, -6.3900e-01, -1.9139e+00, -3.3393e-01],
        [-7.8437e-01,  2.0680e+00, -3.2978e-01, -6.3954e-01, -5.1226e-01],
        [-6.0608e-01,  3.8201e+00, -5.7027e-01, -1.3066e+00,  6.7534e-01],
        [-2.9966e-01,  4.6723e+00, -6.0568e-01, -1.4051e+00,  3.9701e-01],
        [-6.7750e-01,  3.5821e+00, -8.1590e-01, -7.3429e-01, -3.7702e-01],
        [-8.5556e-01,  3.2275e+00, -7.8267e-01, -9.2219e-01, -1.8126e-02],
        [-9.9827e-01,  3.8744e+00, -1.2021e+00, -1.7554e+00,  5.3490e-01],
        [-7.7733e-01,  6.0902e+00, -3.7720e-01, -2.5476e+00,  4.5838e-01],
        [-8.2195e-01,  2.8649e+00, -5.9471e-01, -8.0866e-01,  4.3153e-02],
        [-4.3887e-01,  3.9726e+00, -4.8259e-01, -1.4648e+00,  4.0827e-02],
        [-7.8155e-01,  4.6457e+00, -9.7420e-01, -1.9919e+00,  1.5792e-01],
        [-3.0026e-01,  2.6616e+00, -6.4524e-01, -7.4704e-01, -1.7334e-01],
        [-5.7805e-01,  3.4371e+00, -1.1209e+00, -1.1567e+00, -4.7923e-01],
        [-2.1073e-01,  3.2128e+00, -8.0040e-01, -1.4414e+00, -4.8510e-03],
        [-6.5919e-01,  3.3823e+00, -7.6191e-01, -1.4209e+00, -1.8854e-01],
        [-2.4048e-01,  2.7423e+00, -8.4119e-01, -7.8513e-01, -1.6333e-01],
        [-5.3942e-01,  3.0827e+00, -1.0360e+00, -8.7206e-01,  2.2150e-01],
        [-3.7733e-01,  3.7225e+00, -6.6191e-01, -1.0488e+00,  3.7759e-01],
        [-3.2006e-01,  3.0180e+00, -5.1475e-01, -8.1179e-01, -1.9631e-01],
        [-9.2001e-01,  3.2308e+00, -7.3821e-01, -1.0976e+00,  3.6037e-02],
        [-1.0370e+00,  5.3340e+00, -1.0987e+00, -2.4223e+00,  4.5797e-01],
        [-8.1298e-01,  3.6268e+00, -8.0230e-01, -9.5169e-01, -7.6279e-02],
        [-7.0139e-01,  4.0585e+00, -7.8966e-01, -1.4893e+00,  1.1251e-01],
        [-1.0235e+00,  3.8813e+00, -8.9354e-01, -1.0894e+00,  1.1661e-01],
        [-4.9786e-01,  2.4661e+00, -7.4110e-01, -3.7659e-01,  2.0804e-02],
        [-7.2619e-01,  3.9649e+00, -8.0543e-01, -1.0798e+00,  4.8242e-02],
        [-8.5469e-01,  3.8080e+00, -5.7711e-01, -1.4248e+00,  1.3102e-02],
        [-1.8189e+00,  7.7889e+00, -4.8113e-01, -2.8680e+00, -1.5934e-01],
        [-5.2085e-01,  3.8148e+00, -8.0124e-01, -1.4187e+00, -1.2368e-01],
        [-2.9373e-01,  3.1196e+00, -8.1276e-01, -8.5149e-01,  9.2382e-02],
        [-7.3796e-01,  2.8533e+00, -8.0295e-01, -5.3582e-01, -2.7595e-01],
        [-8.4787e-01,  4.1951e+00, -7.1980e-01, -1.9281e+00,  6.0908e-02],
        [-7.1348e-01,  3.3743e+00, -7.9212e-01, -1.1792e+00,  3.5340e-01],
        [-1.6651e-01,  2.5428e+00, -5.6572e-01, -4.3051e-01, -2.0444e-01],
        [-5.2274e-01,  2.9546e+00, -5.2820e-01, -5.3828e-01,  3.5603e-02],
        [-4.0666e-01,  5.4454e+00, -3.4782e-01, -1.4716e+00,  4.9451e-01],
        [-8.9633e-01,  3.1102e+00, -8.9581e-01, -7.8737e-01, -2.6811e-01],
        [-5.6828e-01,  3.4637e+00, -6.1989e-01, -1.4083e+00, -1.1820e-01],
        [-9.5037e-01,  2.9151e+00, -8.1815e-01, -9.6323e-01,  2.8677e-01],
        [-5.7608e-01,  3.1509e+00, -9.1839e-01, -4.0673e-01, -3.0925e-01],
        [-1.0764e+00,  3.1906e+00, -7.2550e-01, -1.5243e+00,  4.0554e-02],
        [-4.1303e-01,  4.1498e+00, -5.8638e-01, -7.8988e-01, -1.1037e-02],
        [-7.8711e-01,  3.0398e+00, -9.5440e-01, -9.6907e-01, -1.5394e-01],
        [-1.0786e+00,  5.5158e+00, -1.0755e+00, -2.6075e+00,  8.3175e-01],
        [-9.9753e-01,  4.6407e+00, -9.7538e-01, -2.0192e+00,  1.8923e-01],
        [-1.1726e-01,  3.8002e+00, -8.7784e-01, -1.2867e+00,  5.1632e-02],
        [-7.0992e-01,  3.2324e+00, -6.7016e-01, -1.5910e+00,  5.2838e-01],
        [-6.4002e-01,  6.2830e+00, -5.3647e-01, -2.0598e+00,  9.0746e-01],
        [-7.1447e-01,  8.0592e+00, -8.0516e-01, -3.0791e+00,  3.4147e-01],
        [-2.7526e-01,  2.7447e+00, -6.8894e-01, -7.1188e-01, -1.1588e-01],
        [-5.1733e-01,  3.6444e+00, -1.0434e+00, -1.6946e+00, -5.8997e-02],
        [-3.3007e-01,  3.0852e+00, -6.2849e-01, -7.5812e-01, -2.2610e-01],
        [-4.1373e-01,  3.8016e+00, -7.3690e-01, -1.2212e+00,  1.4723e-01],
        [-6.2708e-01,  4.8329e+00, -8.4028e-01, -1.5869e+00,  5.3329e-01],
        [-6.9649e-01,  4.0491e+00, -8.3240e-01, -1.2236e+00,  2.6817e-01],
        [-5.1401e-01,  3.3331e+00, -1.0378e+00, -1.0611e+00, -2.0535e-01],
        [-1.0674e+00,  3.2997e+00, -9.6209e-01, -9.7857e-01,  1.3800e-01],
        [-1.2866e+00,  4.1579e+00, -6.5475e-01, -1.4880e+00,  4.8378e-01],
        [-3.3949e-01,  2.2692e+00, -6.2428e-01, -2.8400e-01, -4.6061e-01],
        [-8.3464e-01,  5.0760e+00, -9.6432e-01, -1.9776e+00,  5.7989e-01],
        [-7.2583e-01,  2.7183e+00, -7.3186e-01, -7.4039e-01, -1.9972e-01],
        [-9.0539e-01,  4.7095e+00, -9.6426e-01, -2.0007e+00, -1.7156e-01],
        [-1.6769e+00,  7.4828e+00, -6.5152e-01, -3.2255e+00,  1.2436e+00],
        [-6.6886e-01,  3.4077e+00, -5.7805e-01, -7.2833e-01,  3.7511e-01],
        [-9.8552e-01,  5.7292e+00, -8.7277e-01, -2.1027e+00,  2.9227e-01],
        [-8.0672e-01,  5.8692e+00, -3.8950e-01, -1.8955e+00,  8.0828e-01],
        [-2.9365e-01,  4.4013e+00, -6.8552e-01, -1.2333e+00, -6.9535e-01],
        [-1.0405e+00,  3.9329e+00, -1.2914e+00, -1.2991e+00, -2.3252e-01],
        [-8.1138e-01,  3.0696e+00, -1.1225e+00, -7.5474e-01, -3.5522e-01],
        [-8.0349e-01,  2.4496e+00, -5.9646e-01, -7.3545e-02, -9.2211e-03],
        [-4.4788e-01,  3.7754e+00, -1.0633e+00, -1.1972e+00, -4.5577e-02],
        [-7.2571e-01,  5.5480e+00, -7.9435e-01, -2.3295e+00,  3.5668e-01],
        [-8.2741e-01,  2.3680e+00, -3.1012e-01, -3.9362e-01,  1.1755e-01],
        [-1.2707e+00,  4.9082e+00, -7.0048e-01, -1.9736e+00,  8.2054e-01],
        [-6.8901e-01,  4.1051e+00, -5.0049e-01, -1.6701e+00,  2.3186e-01],
        [-6.4548e-01,  3.9237e+00, -5.5958e-01, -1.1657e+00, -1.1367e-01],
        [-9.3645e-01,  6.4657e+00, -2.2266e-01, -2.0378e+00,  7.1817e-02],
        [-9.0499e-01,  3.4762e+00, -1.2801e+00, -1.6945e+00,  1.8998e-02],
        [-9.2754e-01,  6.4012e+00, -3.5049e-01, -2.7289e+00,  3.5810e-01],
        [-1.2296e+00,  4.3421e+00, -7.8053e-01, -1.7151e+00,  9.1069e-03],
        [-5.6922e-01,  2.9748e+00, -9.3348e-01, -8.6467e-01, -4.7404e-01],
        [-7.3017e-01,  3.7871e+00, -7.3978e-01, -1.2305e+00,  3.5451e-01],
        [-1.0566e+00,  4.1398e+00, -8.7836e-01, -1.3930e+00,  6.5882e-02],
        [-6.4665e-01,  3.5461e+00, -9.4639e-01, -1.4394e+00,  6.5590e-02],
        [-8.9679e-01,  3.5856e+00, -9.4192e-01, -1.5818e+00, -1.0378e-01],
        [-7.0898e-01,  4.7549e+00, -8.2786e-01, -1.8215e+00,  2.2952e-04],
        [-9.0060e-01,  4.0481e+00, -6.6440e-01, -1.8383e+00,  9.3658e-02],
        [-6.2500e-01,  2.5847e+00, -7.9187e-01, -2.9852e-01,  1.0043e-01],
        [-4.0010e-01,  4.4860e+00, -8.3530e-01, -1.7182e+00, -1.3019e-01],
        [-5.5932e-01,  3.8499e+00, -1.2133e+00, -1.4573e+00,  6.1548e-02],
        [-7.5023e-01,  2.4338e+00, -6.3086e-01, -2.3198e-01, -5.0983e-01],
        [-1.0721e-01,  4.1532e+00, -6.5546e-01, -1.3292e+00,  2.7366e-01],
        [-5.5788e-01,  3.5173e+00, -6.1484e-01, -1.5628e+00,  1.8514e-01],
        [-6.9442e-01,  3.5440e+00, -5.8786e-01, -1.4737e+00,  2.2780e-01],
        [-1.0532e+00,  4.3021e+00, -7.3590e-01, -1.7759e+00,  5.6455e-01],
        [-7.7322e-01,  4.5015e+00, -6.2487e-01, -1.6929e+00,  1.2895e-01],
        [-8.8454e-01,  3.4793e+00, -1.0377e+00, -1.3800e+00, -4.4604e-03],
        [-9.3531e-01,  5.1275e+00, -1.1366e+00, -2.4307e+00,  3.0218e-01],
        [-1.2964e+00,  5.0613e+00, -4.9108e-01, -2.9366e+00,  9.0944e-01],
        [-8.0564e-01,  2.7734e+00, -5.9451e-01, -3.5519e-01, -1.6062e-01],
        [-8.4107e-01,  3.6585e+00, -1.2812e+00, -1.5654e+00,  2.8829e-01],
        [-5.8677e-01,  4.2614e+00, -1.0366e+00, -1.7010e+00,  1.7168e-01],
        [-8.2053e-01,  2.5766e+00, -6.6641e-01, -2.4247e-01,  3.8475e-02],
        [-1.0053e+00,  3.9866e+00, -7.0212e-01, -1.5904e+00,  8.6682e-02],
        [-2.2671e-01,  9.2125e+00, -3.2075e-02, -2.4848e+00,  1.2578e+00],
        [-5.8136e-01,  3.7504e+00, -7.4465e-01, -1.6202e+00, -2.4382e-01],
        [-7.4146e-01,  3.7672e+00, -1.0732e+00, -1.2668e+00, -1.5784e-01],
        [-4.7299e-01,  3.6237e+00, -7.0114e-01, -1.3027e+00,  5.2938e-02]],
       grad_fn=<AddmmBackward0>)
2023-10-26 11:43:08,603 [memo.py] => torch.Size([128, 5])
